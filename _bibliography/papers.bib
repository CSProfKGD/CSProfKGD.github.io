---
---

@string{CVPR = {CVPR}}
@string{ICLR = {ICLR}}
@string{ICCV = {ICCV}}
@string{NeurIPS = {NeurIPS}}

@inproceedings{Dvornik2021,
  abbr={NeurIPS},
  author    = {Nikita Dvornik and Isma Hadji and Konstantinos G. Derpanis and Animesh Garg and Allan D. Jepson},
  title     = {Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers},
  booktitle = NeurIPS,
  year      = {2021},
  pdf       = {https://arxiv.org/abs/2108.11996},
  abstract = {In this work, we consider the problem of sequence-to-sequence alignment for signals containing outliers. Assuming the absence of outliers, the standard Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment between two (generally) variable-length sequences. While DTW is robust to temporal shifts and dilations of the signal, it fails to align sequences in a meaningful way in the presence of outliers that can be arbitrarily interspersed in the sequences. To address this problem, we introduce Drop-DTW, a novel algorithm that aligns the common signal between the sequences while automatically dropping the outlier elements from the matching. The entire procedure is implemented as a single dynamic program that is efficient and fully differentiable. In our experiments, we show that Drop-DTW is a robust similarity measure for sequence retrieval and demonstrate its effectiveness as a training loss on diverse applications. With Drop-DTW, we address temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. In all applications, we take a weakly- or unsupervised approach and demonstrate state-of-the-art results under these settings.},
  selected={true}
}

@inproceedings{abs-2108-07884,
 abbr={ICCV},
  author    = {Md. Amirul Islam and
               Matthew Kowal and
               Sen Jia and
               Konstantinos G. Derpanis and
               Neil D. B. Bruce},
  title     = {Global Pooling, More than Meets the Eye: Position Information is Encoded
               Channel-Wise in CNNs},
  booktitle = ICCV,
  year      = {2021},
  pdf       = {https://arxiv.org/abs/2108.07884},
  abstract = {In this paper, we challenge the common assumption that collapsing the spatial dimensions of a 3D (spatial-channel) tensor in a convolutional neural network (CNN) into a vector via global pooling removes all spatial information. Specifically, we demonstrate that positional information is encoded based on the ordering of the channel dimensions, while semantic information is largely not. Following this demonstration, we show the real world impact of these findings by applying them to two applications. First, we propose a simple yet effective data augmentation strategy and loss function which improves the translation invariance of a CNN's output. Second, we propose a method to efficiently determine which channels in the latent representation are responsible for (i) encoding overall position information or (ii) region-specific positions. We first show that semantic segmentation has a significant reliance on the overall position channels to make predictions. We then show for the first time that it is possible to perform a `region-specific' attack, and degrade a network's performance in a particular part of the input. We believe our findings and demonstrated applications will benefit research areas concerned with understanding the characteristics of CNNs.},
        selected={true}
}

@inproceedings{abs-2105-04551,
 abbr={CVPR},
  author    = {Michael Dorkenwald and
               Timo Milbich and
               Andreas Blattmann and
               Robin Rombach and
               Konstantinos G. Derpanis and
               Bjorn Ommer},
  title     = {Stochastic Image-to-Video Synthesis using cINNs},
  booktitle = CVPR,
  pdf = {https://arxiv.org/abs/2105.04551},
  abstract = {Video understanding calls for a model to learn the characteristic interplay between static scene content and its dynamics: Given an image, the model must be able to predict a future progression of the portrayed scene and, conversely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bijective mapping between the video domain and the static content as well as residual information. In contrast to common stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is naturally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus laying the basis for controlled video synthesis. Experiments on four diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results.},
  year      = {2021},
  html = {https://github.com/CompVis/image2video-synthesis-using-cINNs},
      selected={true}
}

@inproceedings{abs-2105-05217,
 abbr={CVPR},
  author    = {Isma Hadji and
               Konstantinos G. Derpanis and
               Allan D. Jepson},
  title     = {Representation Learning via Global Temporal Alignment and Cycle-Consistency},
  booktitle = CVPR,
  volume    = {abs/2105.05217},
  year      = {2021},
  abstract = {We introduce a weakly supervised method for representation learning based on aligning temporal sequences (e.g., videos) of the same process (e.g., human action). The main idea is to use the global temporal ordering of latent correspondences across sequence pairs as a supervisory signal. In particular, we propose a loss based on scoring the optimal sequence alignment to train an embedding network. Our loss is based on a novel probabilistic path finding view of dynamic time warping (DTW) that contains the following three key features: (i) the local path routing decisions are contrastive and differentiable, (ii) pairwise distances are cast as probabilities that are contrastive as well, and (iii) our formulation naturally admits a global cycle consistency loss that verifies correspondences. For evaluation, we consider the tasks of fine-grained action classification, few shot learning, and video synchronization. We report significant performance increases over previous methods. In addition, we report two applications of our temporal alignment framework, namely 3D pose reconstruction and fine-grained audio/visual retrieval.},
  html = {https://github.com/hadjisma/VideoAlignment},
  pdf       = {https://arxiv.org/abs/2105.05217},
        selected={true}
}

@inproceedings{afifi,
 abbr={CVPR},
  author    = {Mahmoud Afifi and Konstantinos G. Derpanis and Bjorn Ommer and Michael S. Brown},
  title     = {Learning Multi-Scale Photo Exposure Correction},
  booktitle = CVPR,
  year      = {2021},
  abstract = {Capturing photographs with wrong exposures remains a major source of errors in camera-based imaging. Exposure problems are categorized as either: (i) overexposed, where the camera exposure was too long, resulting in bright and washed-out image regions, or (ii) underexposed, where the exposure was too short, resulting in dark regions. Both under- and overexposure greatly reduce the contrast and visual appeal of an image. Prior work mainly focuses on underexposed images or general image enhancement. In contrast, our proposed method targets both over- and underexposure errors in photographs. We formulate the exposure correction problem as two main sub-problems: (i) color enhancement and (ii) detail enhancement. Accordingly, we propose a coarse-to-fine deep neural network (DNN) model, trainable in an end-to-end manner, that addresses each sub-problem separately. A key aspect of our solution is a new dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image. Our method achieves results on par with existing state-of-the-art methods on underexposed images and yields significant improvements for images suffering from overexposure errors.},
  html = {https://github.com/mahmoudnafifi/Exposure_Correction},
  pdf       = {https://arxiv.org/abs/2003.11596},
        selected={true}
}

@inproceedings{IslamKEJODB21,
abbr={ICLR},
  author    = {Md. Amirul Islam and
               Matthew Kowal and
               Patrick Esser and
               Sen Jia and
               Bjorn Ommer and
               Konstantinos G. Derpanis and
               Neil D. B. Bruce},
  title     = {Shape or Texture: Understanding Discriminative Features in CNNs},
  booktitle = ICLR,
  pdf={https://arxiv.org/abs/2101.11604},
  abstract = {Contrasting the previous evidence that neurons in the later layers of a Convolutional Neural Network (CNN) respond to complex object shapes, recent studies have shown that CNNs actually exhibit a `texture bias': given an image with both texture and shape cues (e.g., a stylized image), a CNN is biased towards predicting the category corresponding to the texture. However, these previous studies conduct experiments on the final classification output of the network, and fail to robustly evaluate the bias contained (i) in the latent representations, and (ii) on a per-pixel level. In this paper, we design a series of experiments that overcome these issues. We do this with the goal of better understanding what type of shape information contained in the network is discriminative, where shape information is encoded, as well as when the network learns about object shape during training. We show that a network learns the majority of overall shape information at the first few epochs of training and that this information is largely encoded in the last few layers of a CNN. Finally, we show that the encoding of shape does not imply the encoding of localized per-pixel semantic information. The experimental results and findings provide a more accurate understanding of the behaviour of current CNNs, thus helping to inform future design choices.},
  year      = {2021},
    selected={true}
}


@inproceedings{YuDB20,
 abbr={NeurIPS},
  author    = {Jason J. Yu and
               Konstantinos G. Derpanis and
               Marcus A. Brubaker},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Wavelet Flow: Fast Training of High Resolution Normalizing Flows},
  booktitle = NeurIPS,
  pdf = {https://proceedings.neurips.cc/paper/2020/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html},
  abstract = {Normalizing flows are a class of probabilistic generative models which allow for both fast density computation and efficient sampling and are effective at modelling complex distributions like images. A drawback among current methods is their significant training cost, sometimes requiring months of GPU training time to achieve state-of-the-art results. This paper introduces Wavelet Flow, a multi-scale, normalizing flow architecture based on wavelets. A Wavelet Flow has an explicit representation of signal scale that inherently includes models of lower resolution signals and conditional generation of higher resolution signals, i.e., super resolution. A major advantage of Wavelet Flow is the ability to construct generative models for high resolution data (e.g., 1024 x 1024 images) that are impractical with previous models. Furthermore, Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15Ã— faster to train.},
  year      = {2020},
  html = {https://github.com/YorkUCVIL/Wavelet-Flow},
   selected={true}
}

@inproceedings{DBLP:conf/cvpr/PavlakosZDD17a,
 abbr={CVPR},
  author    = {Georgios Pavlakos and
               Xiaowei Zhou and
               Konstantinos G. Derpanis and
               Kostas Daniilidis},
  title     = {Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose},
  booktitle = CVPR,
  pages     = {1263--1272},
  year      = {2017},
    pdf={https://arxiv.org/abs/1611.07828},
    html = {https://github.com/geopavlakos/c2f-vol-demo},
  abstract = {This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-the-art methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.},
  selected={true}
}