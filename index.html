<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Kosta  Derpanis


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-42ME6JEHET"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-42ME6JEHET');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Kosta</span>  Derpanis
    </h1>
     <p class="desc"><strong>Associate Professor. </strong> CS. Computer Vision. Machine Learning</p>
  </header>
  
  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded-circle" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am an Associate Professor in the <a href="http://eecs.lassonde.yorku.ca">Department of Electical Engineering and Computer Science</a> at <a href="https://www.yorku.ca">York University</a>, Toronto, Canada.  I am also a <a href="https://vectorinstitute.ai">Vector Institute</a> Faculty Affiliate and a Visting Professor at the Samsung AI Centre Toronto.  I am currently serving as an
Associate Editor for the <a href="https://www.computer.org/csdl/journal/tp">IEEE Transactions on Pattern Analysis and Machine Intelligence</a>.</p>

<p>I received an Honours BSc in Computer Science from the <a href="https://web.cs.toronto.edu">University of Toronto</a> and both my MSc (co-supervised by <a href="http://www.cse.yorku.ca/~tsotsos/Tsotsos/Home.html">John Tsotsos</a> and <a href="http://www.cse.yorku.ca/~wildes">Richard Wildes</a>) and PhD (supervised by Richard Wildes) degrees in Computer Science from York University.  I was a postdoc in the <a href="https://www.grasp.upenn.edu">GRASP lab</a> at the <a href="https://www.upenn.edu">University of Pennsylvania</a> with <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>.</p>

<p><strong> Research Interests:</strong> Computer Vision, Deep Learning, Image Processing</p>

<p><strong> I have MSc and PhD openings for Fall 2023.  If interested, please fill out this <a href="https://forms.gle/L3A7U4EDdUTSm8G26">survey</a>.  Due to the high volume nature of the
graduate admissions process, I will not be replying to email inquiries.  If there is a potential fit with my research program, I will contact you.</strong></p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Mar 3, 2022</th>
          <td>
            
              Two papers at CVPR 2022

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 1, 2022</th>
          <td>
            
              Serving as an Area Chair for ECCV 2022 and Social Media Chair for CVPR 2022, ECCV, 2022 and ICCV 2023.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 13, 2021</th>
          <td>
            
              We have funded MSc &amp; PhD openings for Fall 2022: 
<a href="https://twitter.com/CSProfKGD/status/1448317612597854209?s=20">link</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 28, 2021</th>
          <td>
            
              Paper acceped to NeurIPS 2021.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 22, 2021</th>
          <td>
            
              Paper acceped to ICCV 2021.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications (see Google Scholar for full list)</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="Dvornik2021" class="col-sm-8">
    
      <div class="title">Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Dvornik, Nikita,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hadji, Isma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Garg, Animesh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jepson, Allan D.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In NeurIPS</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/abs/2108.11996" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this work, we consider the problem of sequence-to-sequence alignment for signals containing outliers. Assuming the absence of outliers, the standard Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment between two (generally) variable-length sequences. While DTW is robust to temporal shifts and dilations of the signal, it fails to align sequences in a meaningful way in the presence of outliers that can be arbitrarily interspersed in the sequences. To address this problem, we introduce Drop-DTW, a novel algorithm that aligns the common signal between the sequences while automatically dropping the outlier elements from the matching. The entire procedure is implemented as a single dynamic program that is efficient and fully differentiable. In our experiments, we show that Drop-DTW is a robust similarity measure for sequence retrieval and demonstrate its effectiveness as a training loss on diverse applications. With Drop-DTW, we address temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. In all applications, we take a weakly- or unsupervised approach and demonstrate state-of-the-art results under these settings.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCV</abbr>
    
  
  </div>

  <div id="abs-2108-07884" class="col-sm-8">
    
      <div class="title">Global Pooling, More than Meets the Eye: Position Information is Encoded
               Channel-Wise in CNNs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Islam, Md. Amirul,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kowal, Matthew,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jia, Sen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bruce, Neil D. B.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICCV</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/abs/2108.07884" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we challenge the common assumption that collapsing the spatial dimensions of a 3D (spatial-channel) tensor in a convolutional neural network (CNN) into a vector via global pooling removes all spatial information. Specifically, we demonstrate that positional information is encoded based on the ordering of the channel dimensions, while semantic information is largely not. Following this demonstration, we show the real world impact of these findings by applying them to two applications. First, we propose a simple yet effective data augmentation strategy and loss function which improves the translation invariance of a CNN's output. Second, we propose a method to efficiently determine which channels in the latent representation are responsible for (i) encoding overall position information or (ii) region-specific positions. We first show that semantic segmentation has a significant reliance on the overall position channels to make predictions. We then show for the first time that it is possible to perform a `region-specific' attack, and degrade a network's performance in a particular part of the input. We believe our findings and demonstrated applications will benefit research areas concerned with understanding the characteristics of CNNs.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="abs-2105-04551" class="col-sm-8">
    
      <div class="title">Stochastic Image-to-Video Synthesis using cINNs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Dorkenwald, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Milbich, Timo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Blattmann, Andreas,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Rombach, Robin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Ommer, Bjorn
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/abs/2105.04551" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://github.com/CompVis/image2video-synthesis-using-cINNs" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Video understanding calls for a model to learn the characteristic interplay between static scene content and its dynamics: Given an image, the model must be able to predict a future progression of the portrayed scene and, conversely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bijective mapping between the video domain and the static content as well as residual information. In contrast to common stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is naturally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus laying the basis for controlled video synthesis. Experiments on four diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="abs-2105-05217" class="col-sm-8">
    
      <div class="title">Representation Learning via Global Temporal Alignment and Cycle-Consistency</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Hadji, Isma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jepson, Allan D.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/abs/2105.05217" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://github.com/hadjisma/VideoAlignment" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce a weakly supervised method for representation learning based on aligning temporal sequences (e.g., videos) of the same process (e.g., human action). The main idea is to use the global temporal ordering of latent correspondences across sequence pairs as a supervisory signal. In particular, we propose a loss based on scoring the optimal sequence alignment to train an embedding network. Our loss is based on a novel probabilistic path finding view of dynamic time warping (DTW) that contains the following three key features: (i) the local path routing decisions are contrastive and differentiable, (ii) pairwise distances are cast as probabilities that are contrastive as well, and (iii) our formulation naturally admits a global cycle consistency loss that verifies correspondences. For evaluation, we consider the tasks of fine-grained action classification, few shot learning, and video synchronization. We report significant performance increases over previous methods. In addition, we report two applications of our temporal alignment framework, namely 3D pose reconstruction and fine-grained audio/visual retrieval.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="afifi" class="col-sm-8">
    
      <div class="title">Learning Multi-Scale Photo Exposure Correction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Afifi, Mahmoud,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ommer, Bjorn,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Brown, Michael S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/abs/2003.11596" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://github.com/mahmoudnafifi/Exposure_Correction" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Capturing photographs with wrong exposures remains a major source of errors in camera-based imaging. Exposure problems are categorized as either: (i) overexposed, where the camera exposure was too long, resulting in bright and washed-out image regions, or (ii) underexposed, where the exposure was too short, resulting in dark regions. Both under- and overexposure greatly reduce the contrast and visual appeal of an image. Prior work mainly focuses on underexposed images or general image enhancement. In contrast, our proposed method targets both over- and underexposure errors in photographs. We formulate the exposure correction problem as two main sub-problems: (i) color enhancement and (ii) detail enhancement. Accordingly, we propose a coarse-to-fine deep neural network (DNN) model, trainable in an end-to-end manner, that addresses each sub-problem separately. A key aspect of our solution is a new dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image. Our method achieves results on par with existing state-of-the-art methods on underexposed images and yields significant improvements for images suffering from overexposure errors.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="IslamKEJODB21" class="col-sm-8">
    
      <div class="title">Shape or Texture: Understanding Discriminative Features in CNNs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Islam, Md. Amirul,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kowal, Matthew,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Esser, Patrick,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jia, Sen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ommer, Bjorn,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bruce, Neil D. B.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICLR</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/abs/2101.11604" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Contrasting the previous evidence that neurons in the later layers of a Convolutional Neural Network (CNN) respond to complex object shapes, recent studies have shown that CNNs actually exhibit a `texture bias': given an image with both texture and shape cues (e.g., a stylized image), a CNN is biased towards predicting the category corresponding to the texture. However, these previous studies conduct experiments on the final classification output of the network, and fail to robustly evaluate the bias contained (i) in the latent representations, and (ii) on a per-pixel level. In this paper, we design a series of experiments that overcome these issues. We do this with the goal of better understanding what type of shape information contained in the network is discriminative, where shape information is encoded, as well as when the network learns about object shape during training. We show that a network learns the majority of overall shape information at the first few epochs of training and that this information is largely encoded in the last few layers of a CNN. Finally, we show that the encoding of shape does not imply the encoding of localized per-pixel semantic information. The experimental results and findings provide a more accurate understanding of the behaviour of current CNNs, thus helping to inform future design choices.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="YuDB20" class="col-sm-8">
    
      <div class="title">Wavelet Flow: Fast Training of High Resolution Normalizing Flows</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Yu, Jason J.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Brubaker, Marcus A.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In NeurIPS</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://proceedings.neurips.cc/paper/2020/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://github.com/YorkUCVIL/Wavelet-Flow" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Normalizing flows are a class of probabilistic generative models which allow for both fast density computation and efficient sampling and are effective at modelling complex distributions like images. A drawback among current methods is their significant training cost, sometimes requiring months of GPU training time to achieve state-of-the-art results. This paper introduces Wavelet Flow, a multi-scale, normalizing flow architecture based on wavelets. A Wavelet Flow has an explicit representation of signal scale that inherently includes models of lower resolution signals and conditional generation of higher resolution signals, i.e., super resolution. A major advantage of Wavelet Flow is the ability to construct generative models for high resolution data (e.g., 1024 x 1024 images) that are impractical with previous models. Furthermore, Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15Ã— faster to train.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="DBLP:conf/cvpr/PavlakosZDD17a" class="col-sm-8">
    
      <div class="title">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pavlakos, Georgios,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhou, Xiaowei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Derpanis, Konstantinos G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniilidis, Kostas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/abs/1611.07828" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://github.com/geopavlakos/c2f-vol-demo" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-the-art methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6B%6F%73%74%61@%65%65%63%73.%79%6F%72%6B%75.%63%61"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=3Br8x_gAAAAJ&hl=en&oi=ao" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>



<a href="https://www.linkedin.com/in/kosta-derpanis-a07824122" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/CSProfKGD" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Kosta  Derpanis.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
